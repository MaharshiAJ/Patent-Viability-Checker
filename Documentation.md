# Milestone-1:
The most simple task and very self explanatory. The process simply consisted of installing git and creating the github repository. Then pushing the written README.md to the repository. 
# Milestone-2:
Here step 1 was to create a huggingface account and space. Once this is created, a workflow file needed to be created on github to sync the two repositories so that once a push to github occurs the workflow activates and pushes to huggingface using the api tokens. To accomplish this a folder needs to be made called ".github/workflows". Inside this folder a file with the extension .yaml needs to be created. Inside the file the workflow can be written. The template for the code can be found at, https://huggingface.co/docs/hub/spaces-github-actions.

The next step is to copy the default README.md content from the huggingface space into the current README inside of the github repository. Originally I copied it to the bottom of the file after my original Milestone-1 entry however there was an issue on huggingface where the application was not being detected. I then tried putting it at the top and putting Milestone-2 as the text above it however this did not work either. Finally I put the huggingface information at the top and below it I indicated that the text was for milestone-2 and this finally worked. Reading through the documentation I realized that the text at the top of the readme file indicates important information for the huggingface space such as the main app file name, the sdk version, etc. 

After this setup, the rest of this task was simple. Following the Huggingface documentation on transformers the app.py file can be filled out. The process for sentiment analysis is to 
- Go to huggingface model hub and filter by sentiment analysis models and random ones were chosen.
- Create a pipeline object defining the type of classification that will be done, in this case the type is "sentiment-analysis". 
- Pass any text into the pipeline to get the output
  
Once this simple process was done I wanted to add the markdown styling to make it more into a web application. This involves adding a title for what the application is. A drop down menu to select one of the random models I chose. Then a text input box to write down some text to find the sentiment of. Lastly a submit button was added. To implement the button's functionality, the pipeline process from above was moved into a function and the arugments for the function was passed in using the args parameter of the button which was found using the documentation. The last addition made was using the session state object in streamlit to be able to display the output from the pipeline. To get this to work, the session state had to be defined and then in the function, when the outputs are obtained, they are stored in the session and then outside of the session a markdown containing the session information is added. The benefit of this is whenever the session state updates, the markdown updates automatically as well. After this the branch is pushed to github and with the workflow defined the push happens to the huggingface space as well and the app can be viewed. In between I did some pushes to be able to test the changes I was incorporating and found that the while the changes were pushed the changes were not loading on huggingface and found out that sometimes the space needed to be restarted from the settings to clear the cached information.

# Milestone-3
For this milestone the huggingface transformers documentation had to be followed. A google colab notebook was created since google colab provides free access to the GPU which is important when training a model. The github for the Harvard patent dataset contained information on how to download the dataset into the note book. After this I had to find a base model for training. Originally I went with a RoBERTa base model. I found that the training was taking extremely long with the terminal saying that it was going to take 20 hours! I checked the google colab runtime type and saw that for some reason it had been reset to cpu and this was clearly the reason why. I changed to GPU and ran again. The time however was still saying hours. After reading through the RoBERTa model page I saw that it is a very large model with a lot of trainable paramters so instead I read through a research paper that instead proposed using a DistilBERT model instead. It is a much smaller model with a smaller memory footprint and the paper had found that the accuracy of the model was also better. I also saw in the paper that they proposed a model called Longformer-4096 which had a much larger max token size which theoretically is better for something like patent viability analysis because the inputs are usually paragraphs however due to allowing larger tokens, it also increased the memory footprint greatly and the free resources from google colab were not enough to handle training the model so instead I went with the DistilBERT model. Training with this I started with default paramters training with 8 epochs and saw that the validation loss always kept rising which is usually an indication of overfitting and the accuracy never jumped above 45%. I switched some paramters around such as weight decay and optimization and even mixed precision training to help speed up training by reducing memory footprint and managed to get the accuracy up to 55% with low training loss but again massive validation loss. I saw on the research paper that they also managed to get the accuracy up to 60% so decided to end the training process here and exported the checkpoint file with the best results into my repository. This was pushed to github. After this I tested my application on my local machine by running streamlit run app.py in windows powershell and this ran fine in my browser. However, on huggingface an error arises with no error message on why or how to fix the issue. I captured a video testing my application in order to get the video for milestone-4 and pushed that video as well and saw on github that .mp4 files cannot be pushed to huggingface. I instead disabled my workflow and commented it out and manually uploaded the files to huggingface and still the error presisted. Not knowing how to solve this I decided to just allow my video be the evidence that I completed the project with success. Even through google I was not able to figure out why the error was occuring or how to fix it with some people saying it was just an error on huggingface's end. 

# Huggingface error
![Error Message](HuggingFace%20Error.png)

hf.co simply redirected to the huggingface main page with no actual information on the error.